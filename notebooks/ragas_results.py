#!/usr/bin/env python
# coding: utf-8

# # RAG Evaluation Without Ground Truth ‚Äî Experiments
# 
# **M·ª•c ti√™u:** Th·ª≠ nghi·ªám c√°c framework evaluation kh√¥ng c·∫ßn `expected_answer` v√† `ground_truth_context`
# 
# ## V·∫•n ƒë·ªÅ
# 
# H·ªá th·ªëng hi·ªán t·∫°i ph·ª• thu·ªôc v√†o:
# - ‚ùå `expected_answer` ‚Äî kh√¥ng c√≥ s·∫µn trong th·ª±c t·∫ø
# - ‚ùå `ground_truth_context` ‚Äî kh√¥ng c√≥ v√¨ RAG t·ª± ƒë·ªông retrieve
# 
# ## Gi·∫£i ph√°p
# 
# Test 3 frameworks:
# 1. **DeepEval** ‚Äî LLM-as-Judge (no ground truth mode)
# 2. **RAGAS** ‚Äî RAG-specific metrics
# 3. **OpenRAG-Eval** (Optional) ‚Äî Research approach
# 
# ## Workflow
# 
# 1. Load test cases
# 2. Generate answers from RAG API
# 3. Evaluate with each framework
# 4. Meta-evaluate: So s√°nh consistency
# 5. Human-in-the-loop validation
# 
# ---

# ## üì¶ Setup & Installation
# 
# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt

# In[2]:


get_ipython().system('pip install ragas datasets langchain langchain-community langchain-openai')
get_ipython().system('pip install deepeval pandas matplotlib seaborn scipy')


# In[3]:


# Install packages n·∫øu c·∫ßn (uncomment)
# !pip install ragas datasets langchain langchain-community langchain-openai
# !pip install deepeval pandas matplotlib seaborn scipy

import os, sys, warnings, json, pandas as pd, numpy as np
from typing import Dict, List, Any
from dotenv import load_dotenv

# Suppress warnings
warnings.filterwarnings('ignore')

# Add project root to path
sys.path.append(os.path.abspath('.'))

# Load .env v√† ki·ªÉm tra API key
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("B·∫°n ch∆∞a ƒë·∫∑t OPENAI_API_KEY trong file .env")

print("‚úÖ Setup complete!")


# ## 1Ô∏è‚É£ Load Test Cases
# 
# Load v√† explore test cases t·ª´ `data/testcases.json`

# In[4]:


# Load test cases
with open('../data/testcase/factual_testcase/results.json', 'r', encoding='utf-8-sig') as f:
    results = json.load(f)

print(f"üìä Loaded {len(results)} test cases\n")

# Display first test case
print("Example test case:")
for  testcase in results[:2]:
    print(f"ID: {testcase['question_id']}")
    print(f"Question: {testcase['question']}")



# ## 3Ô∏è‚É£ RAGAS Evaluation (No Ground Truth)
# 
# RAGAS c√≥ c√°c metrics kh√¥ng c·∫ßn `expected_answer`:
# - **Faithfulness** ‚Äî Answer c√≥ faithful v·ªõi retrieved context kh√¥ng?
# - **Answer Relevancy** ‚Äî Answer c√≥ relevant v·ªõi question kh√¥ng?
# 
# C·∫£ 2 metrics n√†y ƒë·ªÅu reference-free!

# In[5]:


get_ipython().system(' pip install langchain_openai')


# In[6]:


# Setup RAGAS with Ollama (local LLM)
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Initialize Ollama LLM for RAGAS
ollama_llm = ChatOpenAI(model="gpt-4o-mini")
ollama_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

ragas_llm = LangchainLLMWrapper(ollama_llm)
ragas_embeddings = LangchainEmbeddingsWrapper(ollama_embeddings)


# In[7]:


import json
from ragas import evaluate
from datasets import Dataset

file_path = "../data/testcase/factual_testcase/results.json"

with open(file_path, "r", encoding="utf-8-sig") as f:
    results = json.load(f)

# L·ªçc nh·ªØng tr∆∞·ªùng h·ª£p c√≥ answer h·ª£p l·ªá
valid_results = [r for r in results if r.get("answer")]

# Chu·∫©n ho√° contexts: RAGAS y√™u c·∫ßu contexts l√† list[str], nh∆∞ng gi·ªØ c·∫£ title
def normalize_contexts(ctx_list):
    if not ctx_list:
        return []
    # Tr·∫£ v·ªÅ list[str] theo format [title:{title}|content:{content}]
    return [f"[title:{c.get('title','')}|content:{c.get('content','')}]" for c in ctx_list]


ragas_data = {
    "question": [r["question"] for r in valid_results],
    "answer": [r["answer"] for r in valid_results],
    "contexts": [normalize_contexts(r["contexts"]) for r in valid_results],
    "ground_truth": [r.get("ground_truth", "") for r in valid_results],  # üî• th√™m ground truth
}

ragas_dataset = Dataset.from_dict(ragas_data)


# In theo t·ª´ng c√¢u h·ªèi k√®m content, answer v√† ground_truth
for i, r in enumerate(valid_results):
    print(f"C√¢u h·ªèi: {r['question']}\n")

    print("N·ªôi dung:")
    for c in normalize_contexts(r["contexts"]):
        print(f"- {c}\n")  # xu·ªëng d√≤ng sau m·ªói content

    print(f"Answer: {r['answer']}\n")
    print(f"Ground truth: {r.get('ground_truth', '')}\n")

    print("="*50 + "\n")  # ph√¢n c√°ch gi·ªØa c√°c c√¢u h·ªèi



# In[8]:


# ---------------------------------------------------
# 2Ô∏è‚É£ Import metrics async
# ---------------------------------------------------

from ragas.metrics import AnswerCorrectness, AnswerRelevancy, Faithfulness, ContextRelevance, ContextRecall
import pandas as pd

metrics_dict = {
    "AnswerCorrectness": AnswerCorrectness(llm=ragas_llm),
    "AnswerRelevancy": AnswerRelevancy(embeddings=ragas_embeddings),
    "Faithfulness": Faithfulness(llm=ragas_llm),
    "ContextRelevance": ContextRelevance(llm=ragas_llm),
    "ContextRecall": ContextRecall()
}



# In[9]:


# ---------------------------------------------------
# 2Ô∏è‚É£ Import c√°c metric async-compatible
# ---------------------------------------------------

from ragas.metrics import AnswerCorrectness, AnswerRelevancy, Faithfulness, ContextRelevance, ContextRecall
import pandas as pd

metrics_dict = {
    "AnswerCorrectness": AnswerCorrectness(llm=ragas_llm),
    "AnswerRelevancy": AnswerRelevancy(embeddings=ragas_embeddings),
    "Faithfulness": Faithfulness(llm=ragas_llm),
    "ContextRelevance": ContextRelevance(llm=ragas_llm),
    "ContextRecall": ContextRecall()
}

summary_scores = {}

for name, metric in metrics_dict.items():
    print(f"\nüöÄ Running metric: {name} ...")

    result = evaluate(
        dataset=ragas_dataset,
        metrics=[metric],
        llm=ragas_llm,
        embeddings=ragas_embeddings,
        batch_size=5,
        show_progress=True
    )

    score = list(result._scores_dict.values())[0]
    summary_scores[name] = score

df_scores = pd.DataFrame([summary_scores])
print("\n‚úÖ Summary scores preview:")
print(df_scores)


# In[12]:


import numpy as np

# T·∫°o dict ƒë·ªÉ l∆∞u trung b√¨nh
avg_scores = {}

for col in df_scores.columns:
    avg_scores[col] = np.mean(df_scores[col][0])  # df_scores[col][0] l√† list score

# Xu·∫•t k·∫øt qu·∫£
print("\n‚úÖ Average score for each metric:")
for metric, avg in avg_scores.items():
    print(f"{metric}: {avg:.4f}")


# In[15]:


import pandas as pd
import csv

# Xu·∫•t df_scores v·ªõi UTF-8 v√† quoting
df_scores.to_csv(
    "summary_scores.csv",      # t√™n file xu·∫•t ra
    index=False,               # kh√¥ng xu·∫•t c·ªôt index
    encoding="utf-8-sig",      # utf-8-sig gi√∫p Excel ƒë·ªçc ƒë√∫ng ti·∫øng Vi·ªát
    quoting=csv.QUOTE_ALL      # ƒë·∫∑t t·∫•t c·∫£ gi√° tr·ªã trong d·∫•u ngo·∫∑c k√©p
)

print("‚úÖ df_scores ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o summary_scores.csv v·ªõi UTF-8 v√† quoting")


# In[17]:


import pandas as pd
import ast

# ƒê·ªçc CSV hi·ªán t·∫°i
df = pd.read_csv("summary_scores.csv")

# H√†m chuy·ªÉn chu·ªói list th√†nh list Python
def parse_list(s):
    if pd.isna(s):
        return []
    s = s.replace("np.float64(", "").replace(")", "")
    return ast.literal_eval(s)

# Chuy·ªÉn t·∫•t c·∫£ c√°c c·ªôt t·ª´ chu·ªói list sang list Python
for col in df.columns:
    df[col] = df[col].apply(parse_list)

# Gi·∫£ s·ª≠ t·∫•t c·∫£ c√°c list trong 1 h√†ng ƒë·∫ßu ti√™n, flatten m·ªçi c·ªôt
num_rows = len(df.iloc[0,0])  # s·ªë ph·∫ßn t·ª≠ trong list ƒë·∫ßu ti√™n
data = {col: df[col].iloc[0] for col in df.columns}

# T·∫°o DataFrame m·ªõi v·ªõi m·ªói ph·∫ßn t·ª≠ 1 h√†ng
df_flat = pd.DataFrame(data)

# L∆∞u CSV m·ªõi, m·ªói ph·∫ßn t·ª≠ 1 h√†ng
df_flat.to_csv("summary_scores.csv", index=False, encoding="utf-8-sig")

print("‚úÖ ƒê√£ xu·∫•t CSV d·∫°ng list t·ª´ tr√™n xu·ªëng cho t·∫•t c·∫£ c√°c c·ªôt")

