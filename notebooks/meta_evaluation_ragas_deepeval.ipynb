{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf423ae",
   "metadata": {},
   "source": [
    "## üì¶ 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "\n",
    "# Setup seaborn style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "\n",
    "print(\"‚úÖ Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf9a9d",
   "metadata": {},
   "source": [
    "## üìÇ 2. Load Sample Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to results files\n",
    "# Change these paths to match your actual result files\n",
    "RAGAS_RESULTS_PATH = '../data/open_rag_eval_results_raptor/eval_results_raptor.json'\n",
    "DEEPEVAL_RESULTS_PATH = '../data/open_rag_eval_results_raptor/deepeval_results.json'\n",
    "\n",
    "# Check if files exist\n",
    "ragas_exists = Path(RAGAS_RESULTS_PATH).exists()\n",
    "deepeval_exists = Path(DEEPEVAL_RESULTS_PATH).exists()\n",
    "\n",
    "print(f\"RAGAS results: {'‚úÖ Found' if ragas_exists else '‚ùå Not found'}\")\n",
    "print(f\"DeepEval results: {'‚úÖ Found' if deepeval_exists else '‚ùå Not found'}\")\n",
    "\n",
    "if not (ragas_exists and deepeval_exists):\n",
    "    print(\"\\n‚ö†Ô∏è Please run RAGAS and DeepEval evaluation first:\")\n",
    "    print(\"  1. python scripts/evaluate_ragas.py\")\n",
    "    print(\"  2. python scripts/evaluate_deepeval.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a47161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAGAS results\n",
    "with open(RAGAS_RESULTS_PATH, 'r', encoding='utf-8-sig') as f:\n",
    "    ragas_data = json.load(f)\n",
    "\n",
    "# Load DeepEval results\n",
    "with open(DEEPEVAL_RESULTS_PATH, 'r', encoding='utf-8-sig') as f:\n",
    "    deepeval_data = json.load(f)\n",
    "\n",
    "print(f\"üìä Loaded {len(ragas_data)} RAGAS results\")\n",
    "print(f\"üìä Loaded {len(deepeval_data)} DeepEval results\")\n",
    "\n",
    "# Show sample structure\n",
    "print(\"\\nüìã RAGAS sample:\")\n",
    "if ragas_data:\n",
    "    print(json.dumps(ragas_data[0], indent=2, ensure_ascii=False)[:300])\n",
    "\n",
    "print(\"\\nüìã DeepEval sample:\")\n",
    "if deepeval_data:\n",
    "    print(json.dumps(deepeval_data[0], indent=2, ensure_ascii=False)[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e292432",
   "metadata": {},
   "source": [
    "## üîó 3. Merge & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dictionaries keyed by testcase_id\n",
    "ragas_dict = {str(r.get('testcase_id', '')): r for r in ragas_data}\n",
    "deepeval_dict = {str(d.get('testcase_id', '')): d for d in deepeval_data}\n",
    "\n",
    "# Find common testcase IDs\n",
    "common_ids = set(ragas_dict.keys()) & set(deepeval_dict.keys())\n",
    "print(f\"\\nüìä Testcase ID Analysis:\")\n",
    "print(f\"  Total RAGAS: {len(ragas_dict)}\")\n",
    "print(f\"  Total DeepEval: {len(deepeval_dict)}\")\n",
    "print(f\"  Common IDs: {len(common_ids)}\")\n",
    "print(f\"  RAGAS only: {len(set(ragas_dict.keys()) - set(deepeval_dict.keys()))}\")\n",
    "print(f\"  DeepEval only: {len(set(deepeval_dict.keys()) - set(ragas_dict.keys()))}\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "rows = []\n",
    "for tc_id in sorted(common_ids):\n",
    "    r = ragas_dict[tc_id]\n",
    "    d = deepeval_dict[tc_id]\n",
    "    \n",
    "    row = {\n",
    "        'testcase_id': tc_id,\n",
    "        'ragas_faithfulness': r.get('faithfulness'),\n",
    "        'ragas_answer_relevancy': r.get('answer_relevancy'),\n",
    "        'deepeval_faithfulness': d.get('faithfulness'),\n",
    "        'deepeval_answer_relevancy': d.get('answer_relevancy'),\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(rows)\n",
    "print(f\"\\n‚úÖ Created comparison dataframe with {len(comparison_df)} rows\")\n",
    "print(f\"\\n{comparison_df.head(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90de68",
   "metadata": {},
   "source": [
    "## üìä 4. Analyze Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aceb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"üìà SUMMARY STATISTICS:\\n\")\n",
    "print(comparison_df[[\n",
    "    'ragas_faithfulness',\n",
    "    'ragas_answer_relevancy',\n",
    "    'deepeval_faithfulness',\n",
    "    'deepeval_answer_relevancy'\n",
    "]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2236c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# RAGAS Faithfulness\n",
    "valid_ragas_faith = comparison_df['ragas_faithfulness'].dropna()\n",
    "axes[0, 0].hist(valid_ragas_faith, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].set_title('RAGAS Faithfulness Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axvline(valid_ragas_faith.mean(), color='red', linestyle='--', label=f'Mean: {valid_ragas_faith.mean():.3f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# DeepEval Faithfulness\n",
    "valid_deepeval_faith = comparison_df['deepeval_faithfulness'].dropna()\n",
    "axes[0, 1].hist(valid_deepeval_faith, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].set_title('DeepEval Faithfulness Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axvline(valid_deepeval_faith.mean(), color='red', linestyle='--', label=f'Mean: {valid_deepeval_faith.mean():.3f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# RAGAS Answer Relevancy\n",
    "valid_ragas_rel = comparison_df['ragas_answer_relevancy'].dropna()\n",
    "axes[1, 0].hist(valid_ragas_rel, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1, 0].set_title('RAGAS Answer Relevancy Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axvline(valid_ragas_rel.mean(), color='red', linestyle='--', label=f'Mean: {valid_ragas_rel.mean():.3f}')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# DeepEval Answer Relevancy\n",
    "valid_deepeval_rel = comparison_df['deepeval_answer_relevancy'].dropna()\n",
    "axes[1, 1].hist(valid_deepeval_rel, bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1, 1].set_title('DeepEval Answer Relevancy Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axvline(valid_deepeval_rel.mean(), color='red', linestyle='--', label=f'Mean: {valid_deepeval_rel.mean():.3f}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Distributions plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde5986",
   "metadata": {},
   "source": [
    "## üîó 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations for each metric\n",
    "print(\"üìä CORRELATION ANALYSIS:\\n\")\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "# Faithfulness correlation\n",
    "valid_faith = comparison_df[['ragas_faithfulness', 'deepeval_faithfulness']].dropna()\n",
    "if len(valid_faith) >= 2:\n",
    "    pearson_faith, p_faith = pearsonr(valid_faith['ragas_faithfulness'], \n",
    "                                       valid_faith['deepeval_faithfulness'])\n",
    "    spearman_faith, sp_faith = spearmanr(valid_faith['ragas_faithfulness'], \n",
    "                                          valid_faith['deepeval_faithfulness'])\n",
    "    \n",
    "    correlation_results['faithfulness'] = {\n",
    "        'pearson': pearson_faith,\n",
    "        'pearson_p': p_faith,\n",
    "        'spearman': spearman_faith,\n",
    "        'spearman_p': sp_faith,\n",
    "        'n_valid': len(valid_faith)\n",
    "    }\n",
    "    \n",
    "    print(f\"Faithfulness ({len(valid_faith)} valid cases):\")\n",
    "    print(f\"  Pearson r = {pearson_faith:.4f} (p={p_faith:.4f})\")\n",
    "    print(f\"  Spearman œÅ = {spearman_faith:.4f} (p={sp_faith:.4f})\")\n",
    "    print()\n",
    "\n",
    "# Answer Relevancy correlation\n",
    "valid_rel = comparison_df[['ragas_answer_relevancy', 'deepeval_answer_relevancy']].dropna()\n",
    "if len(valid_rel) >= 2:\n",
    "    pearson_rel, p_rel = pearsonr(valid_rel['ragas_answer_relevancy'], \n",
    "                                   valid_rel['deepeval_answer_relevancy'])\n",
    "    spearman_rel, sp_rel = spearmanr(valid_rel['ragas_answer_relevancy'], \n",
    "                                      valid_rel['deepeval_answer_relevancy'])\n",
    "    \n",
    "    correlation_results['answer_relevancy'] = {\n",
    "        'pearson': pearson_rel,\n",
    "        'pearson_p': p_rel,\n",
    "        'spearman': spearman_rel,\n",
    "        'spearman_p': sp_rel,\n",
    "        'n_valid': len(valid_rel)\n",
    "    }\n",
    "    \n",
    "    print(f\"Answer Relevancy ({len(valid_rel)} valid cases):\")\n",
    "    print(f\"  Pearson r = {pearson_rel:.4f} (p={p_rel:.4f})\")\n",
    "    print(f\"  Spearman œÅ = {spearman_rel:.4f} (p={sp_rel:.4f})\")\n",
    "\n",
    "print(\"\\nüìù Interpretation:\")\n",
    "print(\"  r > 0.7: Strong agreement\")\n",
    "print(\"  r 0.4-0.7: Moderate agreement\")\n",
    "print(\"  r < 0.4: Weak agreement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0251cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots showing correlation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Faithfulness scatter plot\n",
    "if len(valid_faith) > 0:\n",
    "    axes[0].scatter(valid_faith['ragas_faithfulness'], \n",
    "                    valid_faith['deepeval_faithfulness'],\n",
    "                    alpha=0.6, s=100, color='blue')\n",
    "    axes[0].plot([0, 1], [0, 1], 'r--', label='Perfect agreement', linewidth=2)\n",
    "    axes[0].set_xlabel('RAGAS Faithfulness', fontsize=12)\n",
    "    axes[0].set_ylabel('DeepEval Faithfulness', fontsize=12)\n",
    "    axes[0].set_title(f'Faithfulness: r={pearson_faith:.3f}', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xlim(-0.05, 1.05)\n",
    "    axes[0].set_ylim(-0.05, 1.05)\n",
    "\n",
    "# Answer Relevancy scatter plot\n",
    "if len(valid_rel) > 0:\n",
    "    axes[1].scatter(valid_rel['ragas_answer_relevancy'], \n",
    "                    valid_rel['deepeval_answer_relevancy'],\n",
    "                    alpha=0.6, s=100, color='green')\n",
    "    axes[1].plot([0, 1], [0, 1], 'r--', label='Perfect agreement', linewidth=2)\n",
    "    axes[1].set_xlabel('RAGAS Answer Relevancy', fontsize=12)\n",
    "    axes[1].set_ylabel('DeepEval Answer Relevancy', fontsize=12)\n",
    "    axes[1].set_title(f'Answer Relevancy: r={pearson_rel:.3f}', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_xlim(-0.05, 1.05)\n",
    "    axes[1].set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Correlation scatter plots generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317d6c9",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è 6. Identify Disagreement Cases (High Discrepancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6859132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate differences\n",
    "comparison_df['faith_diff'] = abs(comparison_df['ragas_faithfulness'] - comparison_df['deepeval_faithfulness'])\n",
    "comparison_df['rel_diff'] = abs(comparison_df['ragas_answer_relevancy'] - comparison_df['deepeval_answer_relevancy'])\n",
    "comparison_df['max_diff'] = comparison_df[['faith_diff', 'rel_diff']].max(axis=1)\n",
    "\n",
    "# Find high discrepancy cases (difference > 0.3)\n",
    "threshold = 0.3\n",
    "high_disc = comparison_df[comparison_df['max_diff'] > threshold].copy()\n",
    "high_disc = high_disc.sort_values('max_diff', ascending=False)\n",
    "\n",
    "print(f\"‚ö†Ô∏è DISCREPANCY ANALYSIS (threshold > {threshold}):\\n\")\n",
    "print(f\"  High-discrepancy cases: {len(high_disc)} / {len(comparison_df)}\")\n",
    "print(f\"  Percentage: {(len(high_disc)/len(comparison_df)*100):.1f}%\\n\")\n",
    "\n",
    "if len(high_disc) > 0:\n",
    "    print(\"üìã Top 10 Most Discrepant Cases:\\n\")\n",
    "    display_cols = ['testcase_id', 'ragas_faithfulness', 'deepeval_faithfulness', \n",
    "                   'faith_diff', 'ragas_answer_relevancy', 'deepeval_answer_relevancy', \n",
    "                   'rel_diff', 'max_diff']\n",
    "    print(high_disc[display_cols].head(10).to_string())\n",
    "else:\n",
    "    print(\"‚úÖ No high-discrepancy cases found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze discrepancy patterns\n",
    "print(\"\\nüìä DISCREPANCY STATISTICS:\\n\")\n",
    "print(f\"Max difference observed: {comparison_df['max_diff'].max():.4f}\")\n",
    "print(f\"Mean difference: {comparison_df['max_diff'].mean():.4f}\")\n",
    "print(f\"Median difference: {comparison_df['max_diff'].median():.4f}\")\n",
    "print(f\"Std deviation: {comparison_df['max_diff'].std():.4f}\")\n",
    "\n",
    "# Distribution of differences\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].hist(comparison_df['faith_diff'].dropna(), bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Faithfulness Differences', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('|RAGAS - DeepEval|')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(comparison_df['rel_diff'].dropna(), bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Answer Relevancy Differences', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('|RAGAS - DeepEval|')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Discrepancy analysis plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c9675",
   "metadata": {},
   "source": [
    "## üìù 7. Generate Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21714da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive report\n",
    "report = []\n",
    "report.append(\"=\" * 80)\n",
    "report.append(\"META-EVALUATION REPORT: RAGAS vs DeepEval\")\n",
    "report.append(\"=\" * 80)\n",
    "\n",
    "report.append(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "report.append(f\"  Total testcases analyzed: {len(comparison_df)}\")\n",
    "\n",
    "report.append(f\"\\nüîó CORRELATION ANALYSIS:\")\n",
    "for metric, stats in correlation_results.items():\n",
    "    metric_label = metric.replace('_', ' ').title()\n",
    "    report.append(f\"\\n  {metric_label}:\")\n",
    "    report.append(f\"    Pearson r = {stats['pearson']:.4f} (p={stats['pearson_p']:.4f})\")\n",
    "    report.append(f\"    Spearman œÅ = {stats['spearman']:.4f} (p={stats['spearman_p']:.4f})\")\n",
    "    report.append(f\"    Valid cases: {stats['n_valid']}\")\n",
    "    \n",
    "    if stats['pearson'] > 0.7:\n",
    "        strength = \"Strong\"\n",
    "    elif stats['pearson'] > 0.4:\n",
    "        strength = \"Moderate\"\n",
    "    else:\n",
    "        strength = \"Weak\"\n",
    "    report.append(f\"    ‚Üí {strength} agreement between frameworks\")\n",
    "\n",
    "report.append(f\"\\n‚ö†Ô∏è DISCREPANCY ANALYSIS (threshold > {threshold}):\")\n",
    "report.append(f\"  High-discrepancy cases: {len(high_disc)} ({(len(high_disc)/len(comparison_df)*100):.1f}%)\")\n",
    "\n",
    "if len(high_disc) > 0:\n",
    "    report.append(f\"\\n  Top 5 Most Discrepant Cases:\")\n",
    "    for idx, (_, row) in enumerate(high_disc.head(5).iterrows(), 1):\n",
    "        report.append(f\"\\n    {idx}. Testcase: {row['testcase_id']}\")\n",
    "        report.append(f\"       Faithfulness: RAGAS={row['ragas_faithfulness']:.3f} vs DeepEval={row['deepeval_faithfulness']:.3f} (diff={row['faith_diff']:.3f})\")\n",
    "        report.append(f\"       Answer Relevancy: RAGAS={row['ragas_answer_relevancy']:.3f} vs DeepEval={row['deepeval_answer_relevancy']:.3f} (diff={row['rel_diff']:.3f})\")\n",
    "\n",
    "report.append(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "report.append(f\"  1. Framework Agreement:\")\n",
    "report.append(f\"     - {'‚úÖ Both frameworks show strong agreement' if correlation_results.get('faithfulness', {}).get('pearson', 0) > 0.7 else '‚ö†Ô∏è Frameworks show weak to moderate agreement - consider ensemble approach'}\")\n",
    "report.append(f\"\\n  2. For Production:\")\n",
    "report.append(f\"     - Use RAGAS for RAG-specific metrics (Faithfulness, Context Recall)\")\n",
    "report.append(f\"     - Use DeepEval for general LLM quality (Relevancy, Toxicity, Bias)\")\n",
    "report.append(f\"     - Ensemble approach: average scores when frameworks disagree\")\n",
    "report.append(f\"\\n  3. Handle Disagreements:\")\n",
    "report.append(f\"     - Review {len(high_disc)} high-discrepancy cases manually\")\n",
    "report.append(f\"     - Validate with human annotations (A/B testing)\")\n",
    "report.append(f\"     - Use disagreement cases as training signal for active learning\")\n",
    "report.append(f\"\\n  4. Next Steps:\")\n",
    "report.append(f\"     - Build ensemble evaluator combining both metrics\")\n",
    "report.append(f\"     - Create feedback loop for continuous calibration\")\n",
    "report.append(f\"     - Scale evaluation to full production dataset\")\n",
    "\n",
    "report.append(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "report_text = \"\\n\".join(report)\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd95612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save report to JSON\n",
    "report_data = {\n",
    "    'summary': {\n",
    "        'n_testcases': len(comparison_df),\n",
    "        'n_high_discrepancy': len(high_disc),\n",
    "        'discrepancy_threshold': threshold,\n",
    "        'high_discrepancy_percentage': round(len(high_disc) / len(comparison_df) * 100, 2)\n",
    "    },\n",
    "    'correlations': {\n",
    "        metric: {\n",
    "            'pearson': stats.get('pearson'),\n",
    "            'pearson_p': stats.get('pearson_p'),\n",
    "            'spearman': stats.get('spearman'),\n",
    "            'spearman_p': stats.get('spearman_p'),\n",
    "            'n_valid': stats.get('n_valid')\n",
    "        }\n",
    "        for metric, stats in correlation_results.items()\n",
    "    },\n",
    "    'high_discrepancy_cases': high_disc[[\n",
    "        'testcase_id',\n",
    "        'ragas_faithfulness',\n",
    "        'deepeval_faithfulness',\n",
    "        'faith_diff',\n",
    "        'ragas_answer_relevancy',\n",
    "        'deepeval_answer_relevancy',\n",
    "        'rel_diff'\n",
    "    ]].to_dict('records'),\n",
    "    'recommendations': [\n",
    "        \"Use RAGAS for RAG-specific metrics\",\n",
    "        \"Use DeepEval for general LLM quality\",\n",
    "        \"Ensemble approach: average scores when frameworks disagree\",\n",
    "        \"Review high-discrepancy cases manually\",\n",
    "        \"Validate with human annotations\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_path = Path('../data/open_rag_eval_results_raptor/meta_comparison_report.json')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Report saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f6e40",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "**Ph·∫ßn meta-evaluation ƒë√£ complete:**\n",
    "- ‚úÖ Loaded RAGAS & DeepEval results\n",
    "- ‚úÖ Computed Pearson & Spearman correlations\n",
    "- ‚úÖ Found high-discrepancy cases\n",
    "- ‚úÖ Generated visualizations (scatter plots, distributions, difference histograms)\n",
    "- ‚úÖ Created comprehensive report with recommendations\n",
    "\n",
    "**K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u:**\n",
    "- JSON report: `data/open_rag_eval_results_raptor/meta_comparison_report.json`\n",
    "\n",
    "**B∆∞·ªõc ti·∫øp theo:**\n",
    "1. Review high-discrepancy cases manually\n",
    "2. Validate v·ªõi human annotations\n",
    "3. Build ensemble evaluator n·∫øu c·∫ßn\n",
    "4. Scale evaluation t·ªõi full dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
